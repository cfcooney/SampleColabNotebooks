{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"less_simple_colab_program.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"k3YZIZP6xEkD","colab_type":"code","outputId":"270bf1a2-7b37-40e4-8acf-8b8515c98690","executionInfo":{"status":"ok","timestamp":1565090890427,"user_tz":-60,"elapsed":24757,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-GINRuPR_sSs/AAAAAAAAAAI/AAAAAAAABuU/hKv4NuwkEzs/s64/photo.jpg","userId":"00544369604155632679"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["\"\"\"\n","Set up for using CoLab GPU and Google drive\n","\"\"\"\n","# !pip install braindecode\n","# !pip install pandas==0.23.0 #needed for functionality of dataframes code\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/ColabProjects/Study_2a/scripts/\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/ColabProjects/Study_2a/scripts\n","Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rk5pyZofxIQk","colab_type":"code","outputId":"abf14fea-54a6-4456-c874-8323e47e54dd","executionInfo":{"status":"ok","timestamp":1565090898837,"user_tz":-60,"elapsed":5297,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-GINRuPR_sSs/AAAAAAAAAAI/AAAAAAAABuU/hKv4NuwkEzs/s64/photo.jpg","userId":"00544369604155632679"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["import numpy as np\n","import pandas as pd\n","print(pd.__version__)\n","from preprocessing import load_subject_eeg, eeg_to_3d, format_data, down_and_normal, balanced_subsample\n","from utils_2 import current_acc\n","import warnings\n","from imblearn.over_sampling import SMOTE, ADASYN\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","import logging  \n","import time\n","import sys \n","from utils import balanced_subsample, current_loss\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","\n","#####import network architectures#####\n","from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n","from braindecode.models.deep4 import Deep4Net\n","from eegnet import EEGNetv4\n","from braindecode.torch_ext.optimizers import AdamW\n","from braindecode.torch_ext.functions import square, safe_log\n","from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or, And\n","from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor, compute_pred_labels_from_trial_preds\n","from braindecode.torch_ext.constraints import MaxNormDefaultConstraint \n","from experiment_sans_test import Experiment \n","from experiment import Experiment as op_exp # experiemnt for saving optimized models\n","from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor \n","from braindecode.datautil.iterators import BalancedBatchSizeIterator\n","from braindecode.datautil.signal_target import SignalAndTarget\n","from braindecode.torch_ext.util import set_random_seeds, np_to_var, var_to_np\n","\n","from torch.nn.functional import elu, relu6, leaky_relu, relu, rrelu\n","import torch \n","import torch.nn.functional as F\n","from torch.nn.functional import cross_entropy\n","from torch.nn.functional import nll_loss\n","from torch import optim\n","\n","from tensorflow.keras.utils import normalize\n","torch.backends.cudnn.deterministic = True\n","\n","log = logging.getLogger(__name__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.23.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1qGSrd71xEkJ","colab_type":"code","colab":{}},"source":["def results_df(index, index_name, columns_list, column_names):\n","    \"\"\"\n","    create tiered dataframe for hyper-parameter results.\n","    \"\"\"\n","    assert len(columns_list) == len(column_names), \"Unequal length for columns/names!\"\n","    miindex = pd.MultiIndex.from_product([index],names=[index_name])\n","    micol = pd.MultiIndex.from_product(columns_list,names=column_names)\n","    return pd.DataFrame(index=miindex, columns=micol).sortlevel().sortlevel(axis=1)\n","\n","def param_scores_df(columns_list, index):\n","    \"\"\"\n","    Creates dataframe for storing the mean scores for each hyper-parameter\n","    for each subject. Mean and Std. of each hyper-parameter is then stored for plotting.\n","    \"\"\"\n","    index.append(\"Mean\")\n","    index.append(\"Std.\")\n","    df = pd.DataFrame(index=index, columns=columns_list)\n","    a = df.columns.str.split(', ', expand=True).values\n","\n","    #swap values in NaN and replace NAN to ''\n","    df.columns = pd.MultiIndex.from_tuples([('', x[0]) if pd.isnull(x[1]) else x for x in a])\n","    return df\n","\n","def get_col_list(hyp_params):\n","    \"\"\"\n","    returns a list of lists containing hyper-parameters of XD.\n","    \"\"\"\n","    y = []\n","    for n in range(len(list(hyp_params.keys()))):\n","        a = []\n","        x = hyp_params[list(hyp_params.keys())[n]]\n","        if callable(x[0]):\n","            a.append([x[s].__name__ for s in range(len(x))])\n","            y.append(a[0])\n","        else:\n","            y.append(x)\n","    return y\n","\n","def get_loss_acc_df(hyp_params,index_name,num_folds):\n","    \n","    # 2 -- Main Accruacy/loss DataFrame for innerfold\n","    index = list(n+1 for n in range(num_folds*num_folds))\n","    index.append(\"Mean\")\n","    index.append(\"Std.\")\n","    columns_list = get_col_list(hyp_params)\n","    names = list(hyp_params.keys())\n","\n","    lossdf = results_df(index,index_name,columns_list,names)\n","    accdf  = results_df(index,index_name,columns_list,names)\n","    \n","    return lossdf, accdf\n","\n","def get_results_df(hyp_params,index_name,subjects,num_folds):\n","    \n","    #1 - Final accuracies DataFrame\n","    folds = []\n","    for i in range(1,num_folds+1):\n","        folds.append(f'fold{i}')\n","    final_resultsdf = pd.DataFrame(index=subjects, columns=folds)\n","    \n","    # 3 -- DataFrame for storing best HPs by subject\n","    names = list(hyp_params.keys())\n","    paramsdf = pd.DataFrame(index=subjects, columns=names)\n","    \n","    # 4 -- DataFrame for storing HP-specific mean accuracy scores per subject.\n","    # Hard-coded at present.\n","    columns_list = get_col_list(hyp_params)\n","    col =[f'{list(hyp_params.keys())[0]}, {columns_list[0][0]}',f'{list(hyp_params.keys())[0]}, {columns_list[0][1]}',\n","          f'{list(hyp_params.keys())[0]}, {columns_list[0][2]}',f'{list(hyp_params.keys())[0]}, {columns_list[0][3]}',\n","          f'{list(hyp_params.keys())[1]}, {columns_list[1][0]}',f'{list(hyp_params.keys())[1]}, {columns_list[1][1]}',\n","          f'{list(hyp_params.keys())[1]}, {columns_list[1][2]}',f'{list(hyp_params.keys())[1]}, {columns_list[1][3]}', \n","          f'{list(hyp_params.keys())[2]}, {columns_list[2][0]}',f'{list(hyp_params.keys())[2]}, {columns_list[2][1]}',\n","          f'{list(hyp_params.keys())[2]}, {columns_list[2][2]}',f'{list(hyp_params.keys())[2]}, {columns_list[2][3]}',\n","          f'{list(hyp_params.keys())[3]}, {columns_list[3][0]}',f'{list(hyp_params.keys())[3]}, {columns_list[3][1]}']\n","    paramscoresdf = param_scores_df(col, subjects)\n","    return final_resultsdf, paramsdf, paramscoresdf, subjects\n","\n","def call_model(model_type, activation):\n","    if model_type == 'shallow':\n","        model =  ShallowFBCSPNet(in_chans=n_chans, n_classes=n_classes, input_time_length=input_time_length,\n","                     n_filters_time=40, filter_time_length=25, n_filters_spat=40, \n","                     pool_time_length=75, pool_time_stride=15, final_conv_length='auto',\n","                     conv_nonlin=activation, pool_mode='mean', pool_nonlin=safe_log, \n","                     split_first_layer=True, batch_norm=True, batch_norm_alpha=0.1,\n","                     drop_prob=drop_prob).create_network()\n","       \n","    elif model_type == 'deep':\n","        model = Deep4Net(in_chans=n_chans, n_classes=n_classes, input_time_length=input_time_length,\n","                     final_conv_length='auto', n_filters_time=25, n_filters_spat=25, filter_time_length=10,\n","                     pool_time_length=3, pool_time_stride=3, n_filters_2=50, filter_length_2=10,\n","                     n_filters_3=100, filter_length_3=10, n_filters_4=200, filter_length_4=10,\n","                     first_nonlin=activation, first_pool_mode='max', first_pool_nonlin=safe_log, later_nonlin=activation,\n","                     later_pool_mode='max', later_pool_nonlin=safe_log, drop_prob=0.1, \n","                     double_time_convs=False, split_first_layer=False, batch_norm=True, batch_norm_alpha=0.1,\n","                     stride_before_pool=False).create_network() #filter_length_4 changed from 15 to 10\n","\n","    elif model_type == 'eegnet':\n","        model = EEGNetv4(in_chans=n_chans, n_classes=n_classes, final_conv_length='auto', \n","                     input_time_length=input_time_length, pool_mode='mean', F1=16, D=2, F2=32,\n","                     kernel_length=64, third_kernel_size=(8,4), conv_nonlin=activation, drop_prob=0.5).create_network()\n","        \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hB41rN4wxEkO","colab_type":"code","colab":{}},"source":["subjects = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15'] \n","data_type = 'vowels'\n","\n","model_type = 'eegnet'\n","s = SMOTE(sampling_strategy='minority', random_state=10, k_neighbors=3)\n","fs = 1024\n","dec = 8\n","\n","parameters = dict(best_loss = 100.0,\n","                  batch_size = 64,\n","                  monitors = [LossMonitor(), MisclassMonitor(), RuntimeMonitor()],\n","                  model_constraint = MaxNormDefaultConstraint(),\n","                  max_increase_epochs = 30,\n","                 cuda = True)\n","\n","\"\"\"\n","Instantiate dataframes for storing accuracies and hyper-parameter results. \n","\"\"\"\n","hyp_params = dict(activation = [elu, square, leaky_relu, relu],\n","                  lr=[0.001,0.01,0.1,1],\n","                  epochs=[20,40,60,80],\n","                  loss = [cross_entropy, nll_loss]) # model hyper-parameters\n","num_folds = 4\n","skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=10)\n","index_name = 'Fold'\n","all_outer_accuracies, all_best_scores, all_new_scores = [], [], []\n","hyp_param_means_list = []\n","BestParamsList = []\n","final_resultsdf, paramsdf, paramscoresdf, subjects = get_results_df(hyp_params,index_name,subjects,num_folds)\n","final_resultsdf_1 = final_resultsdf.copy()\n","final_resultsdf_2 = final_resultsdf.copy()\n","subjects = subjects[:-2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"txriI5Z3xEkQ","colab_type":"code","colab":{}},"source":["def train_inner(train_set, val_set, hyp_params, parameters):\n","    \"\"\"\n","    Function for performing training on inner loop and \n","    applying nested hyper-parameters.\n","    \"\"\"\n","    best_loss  = parameters[\"best_loss\"]\n","    batch_size = parameters[\"batch_size\"]\n","    monitors   = parameters[\"monitors\"]\n","    cuda       = parameters[\"cuda\"]\n","    model_constraint    = parameters[\"model_constraint\"]\n","    max_increase_epochs = parameters['max_increase_epochs']\n","\n","    iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n","    val_acc, val_loss = [], []\n","    \n","    for activation in hyp_params['activation']:\n","        for lr in hyp_params['lr']:\n","            for n_epochs in hyp_params['epochs']:\n","              for loss in hyp_params['loss']:\n","                  model = None\n","                  model = call_model('eegnet', activation)\n","\n","                  set_random_seeds(seed=20190629, cuda=cuda)\n","\n","                  if cuda:\n","                      model.cuda()\n","                      torch.backends.cudnn.deterministic = True\n","\n","                  log.info(\"%s model: \".format(str(model)))\n","                  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, eps=1e-8, amsgrad=False)\n","                  stop_criterion = Or([MaxEpochs(n_epochs),\n","                           NoDecrease('valid_misclass', max_increase_epochs)])\n","\n","                  loss_function = loss\n","                  model_loss_function = None\n","\n","                  #####Setup to run the selected model#####\n","                  model_test = Experiment(model, train_set, val_set, test_set=None, iterator=iterator,\n","                                          loss_function=loss_function, optimizer=optimizer,\n","                                          model_constraint=model_constraint, monitors=monitors,\n","                                          stop_criterion=stop_criterion, remember_best_column='valid_misclass',\n","                                          run_after_early_stop=True, model_loss_function=model_loss_function, cuda=cuda)\n","                  model_test.run()\n","\n","                  model_acc = model_test.epochs_df['valid_misclass'].astype('float')\n","                  model_loss = model_test.epochs_df['valid_loss'].astype('float')\n","                  current_val_acc = 1 - current_acc(model_acc)\n","                  current_val_loss = current_loss(model_loss)\n","\n","                  val_acc.append(current_val_acc)\n","                  val_loss.append(current_val_loss)\n","                \n","    return val_loss, val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kHhUXOmxEkS","colab_type":"code","colab":{}},"source":["def train_outer(trainsetlist,testsetlist,BestParams,subject,data_type,model_type):\n","    \n","    test_scores, best_scores, new_scores = [], [], []\n","    best_loss  = parameters[\"best_loss\"]\n","    batch_size = parameters[\"batch_size\"]\n","    monitors   = parameters[\"monitors\"]\n","    cuda       = parameters[\"cuda\"]\n","    model_constraint    = parameters[\"model_constraint\"]\n","    max_increase_epochs = parameters['max_increase_epochs']\n","\n","    iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n","    \n","    #####These are the learned hyper-parameters -- only set once for testing#####\n","   \n","    stop_criterion = MaxEpochs(BestParams[2])\n","    activation = getattr(torch.nn.functional, BestParams[0])\n","    loss_function = getattr(torch.nn.functional, BestParams[3])\n","    model_number = 1\n","    for trainset, testset in zip(trainsetlist, testsetlist):\n","    \n","        \n","        model = None\n","        model = call_model(model_type, activation)\n","        optimizer = optim.Adam(model.parameters(), lr=BestParams[1], weight_decay=0, eps=1e-8, amsgrad=False)\n","        \n","        set_random_seeds(seed=20190629, cuda=cuda)\n","\n","        if cuda:\n","            model.cuda()\n","            torch.backends.cudnn.deterministic = True\n","\n","        log.info(\"%s model: \".format(str(model)))\n","\n","        \n","        model_loss_function = None\n","\n","        #####Setup to run the selected model#####\n","        trainset_X, valset_X, trainset_y, valset_y = train_test_split(trainset.X, trainset.y, test_size=0.2,\n","                                                                      shuffle=True, random_state=42, stratify=trainset.y)\n","        train_set = SignalAndTarget(trainset_X, trainset_y)\n","        val_set = SignalAndTarget(valset_X, valset_y)\n","    \n","        optimised_model = op_exp(model, train_set, val_set, test_set=testset, iterator=iterator,\n","                                loss_function=loss_function, optimizer=optimizer,\n","                                model_constraint=model_constraint, monitors=monitors,\n","                                stop_criterion=stop_criterion, remember_best_column='valid_misclass',\n","                                run_after_early_stop=True, model_loss_function=model_loss_function, cuda=cuda,\n","                                data_type=data_type, subject_id=subject, model_type=model_type, \n","                                model_number=str(model_number))\n","        \n","        optimised_model.run()\n","        \n","        best_accuracy = (1 - np.min(np.array(optimised_model.class_acc)))*100\n","        test_accuracy = round((1 - optimised_model.class_acc.pop())*100,3)\n","        new_accuracy = round((1 - optimised_model.epochs_df['test_misclass'].min())*100,3)\n","        \n","        best_scores.append(best_accuracy)\n","        test_scores.append(test_accuracy) # k accuracy scores for this param set.\n","        new_scores.append(new_accuracy)\n","        model_number += 1\n","    return test_scores, best_scores, new_scores, optimised_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pvw0uS75xEkU","colab_type":"code","outputId":"6e860593-ab7a-4edc-c261-416d25c1ca9d","executionInfo":{"status":"ok","timestamp":1565024848993,"user_tz":-60,"elapsed":24387388,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-GINRuPR_sSs/AAAAAAAAAAI/AAAAAAAABuU/hKv4NuwkEzs/s64/photo.jpg","userId":"00544369604155632679"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start = time.time()\n","for subject in subjects:\n","    print(f\"Training Subject {subject} on {data_type}\")\n","    data_folder = f'..//imagined_speech/S{subject}/post_ica/'\n","    start1 = time.time()\n","    _,w_data,_,w_labels = load_subject_eeg(data_folder) #swap depnding on data type\n","    \n","    data, labels = format_data(w_data,w_labels,data_type,4096) #reshape for CNN\n","    data = down_and_normal(data, dec) #downsample and normalise\n","\n","    drs = data.reshape((data.shape[0],data.shape[1]*data.shape[2])) #2D for SMOTE\n","    X, y = s.fit_resample(drs, labels)\n","    X = X.reshape((X.shape[0],data.shape[1],data.shape[2]))\n","    \n","    unique, counts = np.unique(labels, return_counts=True)\n","    n_classes = len(unique)\n","    n_chans   = int(data.shape[1])\n","    input_time_length = data.shape[2]\n","    lossdf, accdf = get_loss_acc_df(hyp_params,index_name,num_folds)\n","    lossdf.head()\n","    \n","    out_fold_num = 0 # outer-fold number\n","    trainsetlist, testsetlist = [],[]\n","    \n","\n","    inner_fold_acc,inner_fold_loss = [],[]\n","    val_acc = 1\n","    \n","    #####Outer=Fold#####\n","    for inner_ind, outer_index in skf.split(X, y):\n","        inner_fold, outer_fold     = X[inner_ind], X[outer_index]\n","        inner_labels, outer_labels = y[inner_ind], y[outer_index]\n","        out_fold_num += 1\n","        loss_with_params = dict()# for storing param values and losses\n","        in_fold_num = 0 # inner-fold number\n","\n","        trainsetlist.append(SignalAndTarget(inner_fold, inner_labels))\n","        testsetlist.append(SignalAndTarget(outer_fold, outer_labels))\n","\n","        #####Inner-Fold#####\n","        for train_idx, valid_idx in skf.split(inner_fold, inner_labels):\n","            X_Train, X_val = inner_fold[train_idx], inner_fold[valid_idx]\n","            y_train, y_val = inner_labels[train_idx], inner_labels[valid_idx]\n","            train_set = SignalAndTarget(X_Train, y_train)\n","            val_set = SignalAndTarget(X_val, y_val)\n","            in_fold_num += 1\n","            hyp_param_acc, hyp_param_loss = [], []\n","\n","            hyp_param_loss, hyp_param_acc = train_inner(train_set, val_set,hyp_params,parameters)\n","\n","            inner_fold_loss.append(hyp_param_loss)\n","            inner_fold_acc.append(hyp_param_acc)\n","\n","        print(f\"Fold run time: {(time.time()-start1) / 60} minutes\")\n","    ####Assigns each fold to DataFrame and computes mean####\n","    for i,j in enumerate(inner_fold_loss):\n","        lossdf.iloc[i] = j\n","        lossdf.head(6)\n","    lossdf.loc[\"Mean\"].iloc[0] = lossdf.iloc[1:16].mean(axis=0).values\n","    lossdf.loc[\"Std.\"].iloc[0] = lossdf.iloc[1:16].std(axis=0).values\n","    lossdf.to_excel(f\"..//results/S{subject}/{model_type}_{data_type}/HP_loss.xlsx\")\n","\n","    for i,j in enumerate(inner_fold_acc):\n","        accdf.iloc[i] = j\n","        accdf.head(6)\n","    accdf.loc[\"Mean\"].iloc[0] = accdf.iloc[1:16].mean(axis=0).values\n","    accdf.loc[\"Std.\"].iloc[0] = accdf.iloc[1:16].std(axis=0).values\n","    accdf.to_excel(f\"..//results/S{subject}/{model_type}_{data_type}/HP_acc.xlsx\")\n","\n","    #####Finds best hyper-parameter set for subject#####\n","    BestParams = lossdf.columns[lossdf.loc[\"Mean\"].values.argmin()]\n","    BestParamsList.append(list(BestParams))\n","    \n","    columns_list = get_col_list(hyp_params)\n","    hyp_param_means = []\n","    for x in columns_list:\n","        for y in x:\n","            sub_df = accdf[[i for i in accdf.columns if i[0] == y or i[1] == y or i[2] == y]]\n","            hyp_param_means.append(sub_df.loc[\"Mean\"].values.mean())\n","    hyp_param_means_list.append(hyp_param_means)\n","\n","    #####Run best hyper-params on entire inner-fold#####\n","    outer_accuracies, best_scores, new_scores, optimised_model = train_outer(trainsetlist,testsetlist,BestParams,subject,data_type,model_type)\n","    all_outer_accuracies.append(outer_accuracies) #k-fold accuracies for all subjects.\n","    all_best_scores.append(best_scores)\n","    all_new_scores.append(new_scores)\n","    \n","    print(f\"subject run time: {(time.time()-start) / 60} minutes\")\n","\n","for i,j in enumerate(all_outer_accuracies):\n","    final_resultsdf.iloc[i] = j\n","\n","#####Compute Final Mean And Standard Deviation of Outer Fold Results#####\n","final_resultsdf['Mean'] = final_resultsdf.mean(axis=1,skipna=True)\n","final_resultsdf['Std.'] = final_resultsdf.std(axis=1,skipna=True)\n","final_resultsdf.to_excel(f'..//results/{model_type}_{data_type}_final_results.xlsx')\n","\n","for i,j in enumerate(all_best_scores):\n","    final_resultsdf_1.iloc[i] = j\n","\n","#####Compute Final Mean And Standard Deviation of Outer Fold Results#####\n","final_resultsdf_1['Mean'] = final_resultsdf_1.mean(axis=1,skipna=True)\n","final_resultsdf_1['Std.'] = final_resultsdf_1.std(axis=1,skipna=True)\n","final_resultsdf_1.to_excel(f'..//results/{model_type}_{data_type}_final_results_1.xlsx')\n","\n","for i,j in enumerate(all_new_scores):\n","    final_resultsdf_2.iloc[i] = j\n","\n","#####Compute Final Mean And Standard Deviation of Outer Fold Results#####\n","final_resultsdf_2['Mean'] = final_resultsdf_2.mean(axis=1,skipna=True)\n","final_resultsdf_2['Std.'] = final_resultsdf_2.std(axis=1,skipna=True)\n","final_resultsdf_2.to_excel(f'..//results/{model_type}_{data_type}_final_results_2.xlsx')\n","\n","for i,j in enumerate(BestParamsList):\n","    paramsdf.iloc[i] = j\n","paramsdf.to_excel(f\"..//results/{model_type}_{data_type}_params.xlsx\")\n","\n","##### Means of each hyper-parameter#####\n","\n","\n","\n","for i,j in enumerate(hyp_param_means_list):\n","    paramscoresdf.iloc[i] = j\n","paramscoresdf.to_excel(f\"..//results/{model_type}_{data_type}_paramscores.xlsx\")\n","print(f\"run time: {(time.time()-start) / 60} minutes\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Subject 01 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.9249866286913555 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 15.170343367258708 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 23.455295034249623 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 30.88637411991755 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 31.094328184922535 minutes\n","Training Subject 02 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 5.888937064011892 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 11.6874986966451 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 17.43274978796641 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 23.49900647799174 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 54.73709511359532 minutes\n","Training Subject 03 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.3946011940638225 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 14.57887532711029 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 22.00837979714076 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 28.679277698198955 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 83.5871349453926 minutes\n","Training Subject 04 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 5.641532667477926 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 11.55327440102895 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 17.971861545244852 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 23.87020038763682 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 107.61988495190938 minutes\n","Training Subject 05 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.811366804440817 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.419100324312845 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 20.048412903149924 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 26.12866568962733 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 133.98622844219207 minutes\n","Training Subject 06 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.1027605652809145 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 14.009677223364513 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 21.014112663269042 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 27.836992776393892 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 161.98239254554113 minutes\n","Training Subject 07 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.5225813627243046 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.206719907124837 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 19.687029004096985 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 25.56460276444753 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 187.71628708442051 minutes\n","Training Subject 08 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.232553827762604 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 12.624326022466024 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 19.410241889953614 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 25.411933143933613 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 213.29097394943238 minutes\n","Training Subject 09 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.683462011814117 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.88858557542165 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 20.435198215643563 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 27.61863247950872 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 241.0806014418602 minutes\n","Training Subject 10 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.023008422056834 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.611684656143188 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 20.50451749563217 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 27.239591932296754 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 268.4649108966192 minutes\n","Training Subject 11 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.398387610912323 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 14.185014514128367 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 21.15505658388138 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 27.968397720654806 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 296.5854327201843 minutes\n","Training Subject 12 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.773257772127788 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.274000155925751 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 19.852378141880035 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 26.33035631974538 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 323.0460660735766 minutes\n","Training Subject 13 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.721330038706461 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 13.652339919408162 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 20.135908448696135 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 26.379280547300976 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 349.55478875637056 minutes\n","Training Subject 14 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 6.8450182716051735 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 14.144464993476868 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 21.54734296798706 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 27.93216275771459 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 377.64052727619804 minutes\n","Training Subject 15 on vowels\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 7.621236590544383 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 14.744380402565003 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 21.691618422667187 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Fold run time: 28.63861999511719 minutes\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["subject run time: 406.44045574267705 minutes\n","run time: 406.4504487315814 minutes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QJUfep8JpGg4","colab_type":"code","colab":{}},"source":["def train_outer_inter(trainsetlist,testsetlist,BestParams,subject,data_type,model_type):\n","    \n","    test_scores, best_scores, new_scores = [], [], []\n","    best_loss  = parameters[\"best_loss\"]\n","    batch_size = parameters[\"batch_size\"]\n","    monitors   = parameters[\"monitors\"]\n","    cuda       = parameters[\"cuda\"]\n","    model_constraint    = parameters[\"model_constraint\"]\n","    max_increase_epochs = parameters['max_increase_epochs']\n","\n","    iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n","    \n","    #####These are the learned hyper-parameters -- only set once for testing#####\n","   \n","    stop_criterion = MaxEpochs(BestParams[2])\n","    activation = getattr(torch.nn.functional, BestParams[0])\n","    loss_function = getattr(torch.nn.functional, BestParams[3])\n","    model_number = 1\n","    for trainset, testset in zip(trainsetlist, testsetlist):\n","    \n","        \n","        model = None\n","        model = call_model(model_type, activation)\n","        optimizer = optim.Adam(model.parameters(), lr=BestParams[1], weight_decay=0, eps=1e-8, amsgrad=False)\n","        \n","        set_random_seeds(seed=20190629, cuda=cuda)\n","\n","        if cuda:\n","            model.cuda()\n","            torch.backends.cudnn.deterministic = True\n","\n","        log.info(\"%s model: \".format(str(model)))\n","\n","        \n","        model_loss_function = None\n","\n","        #####Setup to run the selected model#####\n","        trainset_X, valset_X, trainset_y, valset_y = train_test_split(trainset.X, trainset.y, test_size=0.2,\n","                                                                      shuffle=True, random_state=42, stratify=trainset.y)\n","        train_set = SignalAndTarget(trainset_X, trainset_y)\n","        val_set = SignalAndTarget(valset_X, valset_y)\n","    \n","        optimised_model = op_exp(model, train_set, val_set, test_set=testset, iterator=iterator,\n","                                loss_function=loss_function, optimizer=optimizer,\n","                                model_constraint=model_constraint, monitors=monitors,\n","                                stop_criterion=stop_criterion, remember_best_column='valid_misclass',\n","                                run_after_early_stop=True, model_loss_function=model_loss_function, cuda=cuda,\n","                                data_type=data_type, subject_id=subject, model_type=f\"{model_type}_inter\", \n","                                model_number=str(model_number))\n","        \n","        optimised_model.run()\n","        \n","        best_accuracy = (1 - np.min(np.array(optimised_model.class_acc)))*100\n","        test_accuracy = round((1 - optimised_model.class_acc.pop())*100,3)\n","        new_accuracy = round((1 - optimised_model.epochs_df['test_misclass'].min())*100,3)\n","        \n","        best_scores.append(best_accuracy)\n","        test_scores.append(test_accuracy) # k accuracy scores for this param set.\n","        new_scores.append(new_accuracy)\n","        model_number += 1\n","    return test_scores, best_scores, new_scores, optimised_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MToTow52qd-K","colab_type":"code","colab":{}},"source":["all_inter_accuracies = []\n","for subject in subjects:\n","    data_folder = f'..//imagined_speech/S{subject}/post_ica/'\n","\n","    w_data,_,w_labels,_ = load_subject_eeg(data_folder)\n","    \n","    data, labels = format_data(w_data,w_labels,data_type,4096) #reshape for CNN\n","    data = down_and_normal(data, dec) #downsample and normalise\n","\n","    drs = data.reshape((data.shape[0],data.shape[1]*data.shape[2])) #2D for SMOTE\n","    X, y = s.fit_resample(drs, labels)\n","    X = X.reshape((X.shape[0],data.shape[1],data.shape[2]))\n","    \n","    unique, counts = np.unique(labels, return_counts=True)\n","    n_classes = len(unique)\n","    n_chans   = int(data.shape[1])\n","    input_time_length = data.shape[2]\n","\n","    lossdf, accdf = get_loss_acc_df(hyp_params,index_name,num_folds)\n","    \n","    out_fold_num = 0 # outer-fold number\n","    trainsetlist, testsetlist = [],[]\n","  \n","    inner_fold_acc = []\n","    val_acc = 1\n","    start = time.time()\n","    #####Outer=Fold#####\n","    for inner_ind, outer_index in skf.split(X, y):\n","        inner_fold, outer_fold     = X[inner_ind], X[outer_index]\n","        inner_labels, outer_labels = y[inner_ind], y[outer_index]\n","        out_fold_num += 1\n","\n","        in_fold_num = 0 # inner-fold number\n","\n","        trainsetlist.append(SignalAndTarget(inner_fold, inner_labels))\n","        testsetlist.append(SignalAndTarget(outer_fold, outer_labels))\n","\n","    \n","    #####Run best hyper-params on entire inner-fold#####\n","    test_accuracy, best_scores, new_scores, _ = train_outer_inter(trainsetlist,testsetlist,BestParamsInter,subject,data_type,model_type)\n","    all_inter_accuracies.append(new_scores) #k-fold accuracies for all subjects.\n","\n","for i,j in enumerate(all_inter_accuracies):\n","    inter_resultsdf.iloc[i] = j\n","\n","# #####Compute Final Mean And Standard Deviation of Outer Fold Results#####\n","inter_resultsdf['Mean'] = inter_resultsdf.mean(axis=1,skipna=True)\n","inter_resultsdf['Std.'] = inter_resultsdf.std(axis=1,skipna=True)\n","inter_resultsdf.to_excel(f'..//results/inter_subjects/{model_type}_{data_type}_final_inter_results.xlsx')"],"execution_count":0,"outputs":[]}]}