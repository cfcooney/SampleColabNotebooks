{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simple_colab_program.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ubZbQGq4Y3IM","colab_type":"code","outputId":"943ad0b1-6cb4-499b-d215-ac224a593cb9","executionInfo":{"status":"ok","timestamp":1572107515256,"user_tz":-60,"elapsed":26757,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB97g3BUIpHFvdx_x5i7HwF_79zx8HwTuO9csKDQA=s64","userId":"00544369604155632679"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["\"\"\"\n","Set up for using CoLab GPU and Google drive\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive') # required to access files in your Google drive\n","\n","%cd /content/drive/My Drive/ColabProjects/Study_2a/scripts/\n","\n","\"\"\"\n","Quick test to ensure GPU is present\n","\"\"\"\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/ColabProjects/Study_2a/scripts\n","Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LF_CjJHH5OAm","colab_type":"text"},"source":["Install packages not already available with Colab -- requires restarting runtime"]},{"cell_type":"code","metadata":{"id":"j4SKAHMc5d4T","colab_type":"code","colab":{}},"source":["\"\"\"\n","Use the !pip install command to retrieve packages not installed\n","\"\"\"\n","!pip install braindecode\n","!pip install pandas==0.23.0 # facilitates version control "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6kfEGIb7gE-","colab_type":"text"},"source":["Alternatively use a try/except clause during import"]},{"cell_type":"code","metadata":{"id":"eXH7a7pZmGJH","colab_type":"code","colab":{}},"source":["try:\n","  import pywt\n","except:\n","   !pip install PyWavelets\n","import pywt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ijddk7v8rnO","colab_type":"text"},"source":["Install all remaining required packages - including those in your own Google\n","drive."]},{"cell_type":"code","metadata":{"id":"G9h3L7km8qDQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from preprocessing import load_subject_eeg, eeg_to_3d, format_data, down_and_normal, balanced_subsample\n","from utils_2 import current_acc\n","import warnings\n","from imblearn.over_sampling import SMOTE, ADASYN\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","import logging  \n","import time\n","import sys \n","from utils import balanced_subsample, current_loss\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","\n","#####import network architectures#####\n","from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n","from braindecode.models.deep4 import Deep4Net\n","#from eegnet import EEGNetv4\n","from braindecode.torch_ext.optimizers import AdamW\n","from braindecode.torch_ext.functions import square, safe_log\n","from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or, And\n","from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor\n","from braindecode.torch_ext.constraints import MaxNormDefaultConstraint \n","from experiment_sans_test import Experiment \n","#from experiment import Experiment as op_exp # experiemnt for saving optimized models\n","from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor \n","from braindecode.datautil.iterators import BalancedBatchSizeIterator\n","from braindecode.datautil.signal_target import SignalAndTarget\n","from braindecode.torch_ext.util import set_random_seeds, np_to_var\n","\n","from torch.nn.functional import elu, relu6, leaky_relu, relu, rrelu\n","import torch \n","import torch.nn.functional as F \n","from torch import optim\n","\n","from tensorflow.keras.utils import normalize\n","torch.backends.cudnn.deterministic = True\n","\n","log = logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGrxBIXq89FC","colab_type":"text"},"source":["Define functions required for storing results and calling models"]},{"cell_type":"code","metadata":{"id":"9bYsIwpFY3IQ","colab_type":"code","colab":{}},"source":["def results_df(index, index_name, columns_list, column_names):\n","    \"\"\"\n","    create tiered dataframe for hyper-parameter results.\n","    \"\"\"\n","    assert len(columns_list) == len(column_names), \"Unequal length for columns/names!\"\n","    miindex = pd.MultiIndex.from_product([index],names=[index_name])\n","    micol = pd.MultiIndex.from_product(columns_list,names=column_names)\n","    return pd.DataFrame(index=miindex, columns=micol).sortlevel().sortlevel(axis=1)\n","\n","def param_scores_df(columns_list, index):\n","    \"\"\"\n","    Creates dataframe for storing the mean scores for each hyper-parameter\n","    for each subject. Mean and Std. of each hyper-parameter is then stored for plotting.\n","    \"\"\"\n","    index.append(\"Mean\")\n","    index.append(\"Std.\")\n","    df = pd.DataFrame(index=index, columns=columns_list)\n","    a = df.columns.str.split(', ', expand=True).values\n","\n","    #swap values in NaN and replace NAN to ''\n","    df.columns = pd.MultiIndex.from_tuples([('', x[0]) if pd.isnull(x[1]) else x for x in a])\n","    return df\n","\n","def get_col_list(hyp_params):\n","    \"\"\"\n","    returns a list of lists containing hyper-parameters of XD.\n","    \"\"\"\n","    y, a = [],[]\n","    for n in range(len(list(hyp_params.keys()))):\n","        x = hyp_params[list(hyp_params.keys())[n]]\n","        if callable(x[0]):\n","            a.append([x[s].__name__ for s in range(len(x))])\n","            y.append(a[0])\n","        else:\n","            y.append(x)\n","    return y\n","\n","def get_results_df(hyp_params,index_name,subjects,num_folds):\n","    \n","    #1 - Final accuracies DataFrame\n","    folds = []\n","    for i in range(1,num_folds+1):\n","        folds.append(f'fold{i}')\n","    final_resultsdf = pd.DataFrame(index=subjects, columns=folds)\n","    \n","    # 2 -- Main Accruacy DataFrame for innerfold\n","    index = list(n+1 for n in range(num_folds*num_folds))\n","    index.append(\"Mean\")\n","    index.append(\"Std.\")\n","    columns_list = get_col_list(hyp_params)\n","    names = list(hyp_params.keys())\n","\n","    lossdf = results_df(index,index_name,columns_list,names)\n","    accdf  = results_df(index,index_name,columns_list,names)\n","    # 3 -- DataFrame for storing best HPs by subject\n","    paramsdf = pd.DataFrame(index=subjects, columns=names)\n","    \n","    # 4 -- DataFrame for storing HP-specific mean accuracy scores per subject.\n","    # Hard-coded at present.\n","    col =[f'{list(hyp_params.keys())[0]}, {columns_list[0][0]}',f'{list(hyp_params.keys())[0]}, {columns_list[0][1]}',\n","          f'{list(hyp_params.keys())[0]}, {columns_list[0][2]}',f'{list(hyp_params.keys())[0]}, {columns_list[0][3]}',\n","          f'{list(hyp_params.keys())[1]}, {columns_list[1][0]}',f'{list(hyp_params.keys())[1]}, {columns_list[1][1]}',\n","          f'{list(hyp_params.keys())[1]}, {columns_list[1][2]}',f'{list(hyp_params.keys())[1]}, {columns_list[1][3]}', \n","          f'{list(hyp_params.keys())[2]}, {columns_list[2][0]}',f'{list(hyp_params.keys())[2]}, {columns_list[2][1]}',\n","          f'{list(hyp_params.keys())[2]}, {columns_list[2][2]}',f'{list(hyp_params.keys())[2]}, {columns_list[2][3]}']\n","    paramscoresdf = param_scores_df(col, subjects)\n","    return final_resultsdf, lossdf, accdf, paramsdf, paramscoresdf, subjects\n","  \n","def call_model(model_type, activation):\n","    if model_type == 'shallow':\n","        model =  ShallowFBCSPNet(in_chans=n_chans, n_classes=n_classes, input_time_length=input_time_length,\n","                     n_filters_time=80, filter_time_length=40, n_filters_spat=80, \n","                     pool_time_length=75, pool_time_stride=25, final_conv_length='auto',\n","                     conv_nonlin=square, pool_mode='max', pool_nonlin=safe_log, \n","                     split_first_layer=True, batch_norm=True, batch_norm_alpha=0.1,\n","                     drop_prob=drop_prob).create_network()\n","       \n","    elif model_type == 'deep':\n","        model = Deep4Net(in_chans=n_chans, n_classes=n_classes, input_time_length=input_time_length,\n","                     final_conv_length='auto', n_filters_time=25, n_filters_spat=25, filter_time_length=10,\n","                     pool_time_length=3, pool_time_stride=3, n_filters_2=50, filter_length_2=10,\n","                     n_filters_3=100, filter_length_3=10, n_filters_4=200, filter_length_4=10,\n","                     first_nonlin=activation, first_pool_mode='mean', first_pool_nonlin=safe_log, later_nonlin=activation,\n","                     later_pool_mode='mean', later_pool_nonlin=safe_log, drop_prob=0.1, \n","                     double_time_convs=False, split_first_layer=True, batch_norm=True, batch_norm_alpha=0.1,\n","                     stride_before_pool=False).create_network() #filter_length_4 changed from 15 to 10\n","\n","    elif model_type == 'eegnet':\n","        model = EEGNetv4(in_chans=n_chans, n_classes=n_classes, final_conv_length='auto', \n","                     input_time_length=input_time_length, pool_mode='mean', F1=16, D=2, F2=32,\n","                     kernel_length=64, third_kernel_size=(8,4), drop_prob=drop_prob).create_network()\n","        \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrGoLrixY3IS","colab_type":"code","colab":{}},"source":["subjects = ['01','02']#,'03','04','05','06','07','08','09','10','11','12','13','14','15'] \n","data_type = 'words'\n","s = SMOTE(sampling_strategy='minority', random_state=10, k_neighbors=3)\n","fs = 1024\n","dec = 8\n","\n","parameters = dict(best_loss = 100.0,\n","                  batch_size = 64,\n","                  monitors = [LossMonitor(), MisclassMonitor(), RuntimeMonitor()],\n","                  model_constraint = MaxNormDefaultConstraint(),\n","                  max_increase_epochs = 30,\n","                  cuda = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndxSCUJzY3IU","colab_type":"code","outputId":"00291e60-d4b4-4329-e373-621204ce17e5","executionInfo":{"status":"ok","timestamp":1561142422829,"user_tz":-60,"elapsed":1631,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-Jh1iaHp4PZg/AAAAAAAAAAI/AAAAAAAAAAc/qA6ApgQsxU8/s64/photo.jpg","userId":"04274758073011560879"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["for subject in subjects:\n","    data_folder = f'..//imagined_speech/S{subject}/post_ica/'\n","\n","    w_data,_,w_labels,_ = load_subject_eeg(data_folder)\n","    \n","    data, labels = format_data(w_data,w_labels,data_type,4096) #reshape for CNN\n","    data = down_and_normal(data, dec) #downsample and normalise\n","\n","    drs = data.reshape((data.shape[0],data.shape[1]*data.shape[2])) #2D for SMOTE\n","    X, y = s.fit_resample(drs, labels)\n","    X = X.reshape((X.shape[0],data.shape[1],data.shape[2]))\n","    \n","    unique, counts = np.unique(labels, return_counts=True)\n","    n_classes = len(unique)\n","    n_chans   = int(data.shape[1])\n","    input_time_length = data.shape[2]\n","    \n","    print(X.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(306, 6, 512)\n","(262, 6, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2rW-3r1AY3IY","colab_type":"code","colab":{}},"source":["\"\"\"\n","Instantiate dataframes for storing accuracies and hyper-parameter results. \n","\"\"\"\n","hyp_params = dict(activation = [elu, relu6, leaky_relu, relu],\n","                  lr=[0.001,0.01,0.1,1],\n","                  epochs=[20,40,60,80]) # model hyper-parameters\n","num_folds = 4\n","index_name = 'Fold'\n","final_resultsdf, lossdf, accdf, paramsdf, paramscoresdf, subjects = get_results_df(hyp_params,index_name,subjects,num_folds)\n","subjects = subjects[:-2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SczKb964Y3Ia","colab_type":"code","colab":{}},"source":["def train_inner(train_set, val_set, hyp_params, parameters):\n","    \"\"\"\n","    Function for performing training on inner loop and \n","    applying nested hyper-parameters.\n","    \"\"\"\n","    best_loss  = parameters[\"best_loss\"]\n","    batch_size = parameters[\"batch_size\"]\n","    monitors   = parameters[\"monitors\"]\n","    cuda       = parameters[\"cuda\"]\n","    model_constraint    = parameters[\"model_constraint\"]\n","    max_increase_epochs = parameters['max_increase_epochs']\n","\n","    iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n","    val_acc, val_loss = [], []\n","    \n","    for activation in hyp_params['activation']:\n","        for lr in hyp_params['lr']:\n","            for n_epochs in hyp_params['epochs']:\n","                model = None\n","                model = call_model('deep', activation)\n","                \n","                set_random_seeds(seed=20190629, cuda=cuda)\n","\n","                if cuda:\n","                    model.cuda()\n","                    torch.backends.cudnn.deterministic = True\n","                \n","                log.info(\"%s model: \".format(str(model)))\n","                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, eps=1e-8, amsgrad=False)\n","                stop_criterion = Or([MaxEpochs(n_epochs),\n","                         NoDecrease('valid_misclass', max_increase_epochs)])\n","                \n","                loss_function = F.cross_entropy\n","                model_loss_function = None\n","               \n","                #####Setup to run the selected model#####\n","                model_test = Experiment(model, train_set, val_set, test_set=None, iterator=iterator,\n","                                        loss_function=loss_function, optimizer=optimizer,\n","                                        model_constraint=model_constraint, monitors=monitors,\n","                                        stop_criterion=stop_criterion, remember_best_column='valid_misclass',\n","                                        run_after_early_stop=True, model_loss_function=model_loss_function, cuda=cuda)\n","             \n","                model_test.run()\n","                model_acc = model_test.epochs_df['valid_misclass'].astype('float')\n","                model_loss = model_test.epochs_df['valid_loss'].astype('float')\n","                current_val_acc = 1 - current_acc(model_acc)\n","                current_val_loss = current_loss(model_loss)\n","                \n","#                 accuracy = 1 - np.min(np.array(optimized_model.class_acc))\n","# \t\t            cv_scores.append(accuracy) # k accuracy scores for this param set. \n","\t\t\n","                \n","                val_acc.append(current_val_acc)\n","                val_loss.append(current_val_loss)\n","    \n","    return val_loss, val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zq0TUMyvY3Ic","colab_type":"code","outputId":"4a4b8e84-2969-44ad-e875-1d164459a61b","executionInfo":{"status":"ok","timestamp":1561144521821,"user_tz":-60,"elapsed":1885960,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-Jh1iaHp4PZg/AAAAAAAAAAI/AAAAAAAAAAc/qA6ApgQsxU8/s64/photo.jpg","userId":"04274758073011560879"}},"colab":{"base_uri":"https://localhost:8080/","height":334}},"source":["skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=10)\n","out_fold_num = 0 # outer-fold number\n","trainsetlist, testsetlist = [],[]\n","cv_scores = []\n","BestParamsList = []\n","\n","\n","inner_fold_acc,inner_fold_loss = [],[]\n","val_acc = 1\n","start = time.time()\n","#####Outer=Fold#####\n","for inner_ind, outer_index in skf.split(X, y):\n","    inner_fold, outer_fold     = X[inner_ind], X[outer_index]\n","    inner_labels, outer_labels = y[inner_ind], y[outer_index]\n","    out_fold_num += 1\n","    loss_with_params = dict()# for storing param values and losses\n","    in_fold_num = 0 # inner-fold number\n","    \n","    trainsetlist.append(SignalAndTarget(inner_fold, inner_labels))\n","    testsetlist.append(SignalAndTarget(outer_fold, outer_labels))\n","   \n","    #####Inner-Fold#####\n","    for train_idx, valid_idx in skf.split(inner_fold, inner_labels):\n","        X_Train, X_val = inner_fold[train_idx], inner_fold[valid_idx]\n","        y_train, y_val = inner_labels[train_idx], inner_labels[valid_idx]\n","        train_set = SignalAndTarget(X_Train, y_train)\n","        val_set = SignalAndTarget(X_val, y_val)\n","        in_fold_num += 1\n","        hyp_param_acc, hyp_param_loss = [], []\n","        \n","        hyp_param_loss, hyp_param_acc = train_inner(train_set, val_set,hyp_params,parameters)\n","        \n","        inner_fold_loss.append(hyp_param_loss)\n","        inner_fold_acc.append(hyp_param_acc)\n","       \n","print(f\"run time: {time.time()-start} seconds\")\n","#####Assigns each fold to DataFrame and computes mean####\n","for i,j in enumerate(inner_fold_loss):\n","    lossdf.iloc[i] = j\n","    lossdf.head(6)\n","lossdf.loc[\"Mean\"].iloc[0] = lossdf.iloc[1:16].mean(axis=0).values\n","lossdf.loc[\"Std.\"].iloc[0] = lossdf.iloc[1:16].std(axis=0).values\n","lossdf.to_excel(f\"..//results/S{'01'}/HP_acc.xlsx\")\n","\n","for i,j in enumerate(inner_fold_acc):\n","    accdf.iloc[i] = j\n","    accdf.head(6)\n","accdf.loc[\"Mean\"].iloc[0] = accdf.iloc[1:16].mean(axis=0).values\n","accdf.loc[\"Std.\"].iloc[0] = accdf.iloc[1:16].std(axis=0).values\n","\n","#####Finds best hyper-parameter set for subject#####\n","BestParams = lossdf.columns[df.loc[\"Mean\"].values.argmin()]\n","BestParamsList.append(list(BestParams))\n","\n","for i,j in enumerate(BestParamsList):\n","    paramsdf.iloc[i] = j\n","\n","##### Means of each hyper-parameter#####\n","columns_list = get_col_list(hyp_params)\n","hyp_param_means_list = []\n","hyp_param_means = []\n","for x in columns_list:\n","    for y in x:\n","        sub_df = accdf[[i for i in accdf.columns if i[0] == y or i[1] == y or i[2] == y]]\n","        hyp_param_means.append(sub_df.loc[\"Mean\"].values.mean())\n","hyp_param_means_list.append(hyp_param_means)\n","    \n","for i,j in enumerate(hyp_param_means_list):\n","    paramscoresdf.iloc[i] = j\n","\n","df.tail()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["run time: 1884.9905281066895 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th>activation</th>\n","      <th colspan=\"16\" halign=\"left\">elu</th>\n","      <th colspan=\"16\" halign=\"left\">leaky_relu</th>\n","      <th colspan=\"16\" halign=\"left\">relu</th>\n","      <th colspan=\"16\" halign=\"left\">relu6</th>\n","    </tr>\n","    <tr>\n","      <th>lr</th>\n","      <th colspan=\"4\" halign=\"left\">0.001</th>\n","      <th colspan=\"4\" halign=\"left\">0.010</th>\n","      <th colspan=\"4\" halign=\"left\">0.100</th>\n","      <th colspan=\"4\" halign=\"left\">1.000</th>\n","      <th colspan=\"4\" halign=\"left\">0.001</th>\n","      <th colspan=\"4\" halign=\"left\">0.010</th>\n","      <th colspan=\"4\" halign=\"left\">0.100</th>\n","      <th colspan=\"4\" halign=\"left\">1.000</th>\n","      <th colspan=\"4\" halign=\"left\">0.001</th>\n","      <th colspan=\"4\" halign=\"left\">0.010</th>\n","      <th colspan=\"4\" halign=\"left\">0.100</th>\n","      <th colspan=\"4\" halign=\"left\">1.000</th>\n","      <th colspan=\"4\" halign=\"left\">0.001</th>\n","      <th colspan=\"4\" halign=\"left\">0.010</th>\n","      <th colspan=\"4\" halign=\"left\">0.100</th>\n","      <th colspan=\"4\" halign=\"left\">1.000</th>\n","    </tr>\n","    <tr>\n","      <th>epochs</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","    </tr>\n","    <tr>\n","      <th>Fold</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>14</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Mean</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Std.</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["activation   elu                           ...  relu6                          \n","lr         0.001                0.010      ...  0.100      1.000               \n","epochs        20   40   60   80    20   40 ...     60   80    20   40   60   80\n","Fold                                       ...                                 \n","14           NaN  NaN  NaN  NaN   NaN  NaN ...    NaN  NaN   NaN  NaN  NaN  NaN\n","15           NaN  NaN  NaN  NaN   NaN  NaN ...    NaN  NaN   NaN  NaN  NaN  NaN\n","16           NaN  NaN  NaN  NaN   NaN  NaN ...    NaN  NaN   NaN  NaN  NaN  NaN\n","Mean         NaN  NaN  NaN  NaN   NaN  NaN ...    NaN  NaN   NaN  NaN  NaN  NaN\n","Std.         NaN  NaN  NaN  NaN   NaN  NaN ...    NaN  NaN   NaN  NaN  NaN  NaN\n","\n","[5 rows x 64 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"aBtdOpEHY3Il","colab_type":"code","outputId":"d97481f6-ff28-4841-e32e-aaf098289024","executionInfo":{"status":"ok","timestamp":1561145778819,"user_tz":-60,"elapsed":383,"user":{"displayName":"Ciaran Cooney","photoUrl":"https://lh5.googleusercontent.com/-Jh1iaHp4PZg/AAAAAAAAAAI/AAAAAAAAAAc/qA6ApgQsxU8/s64/photo.jpg","userId":"04274758073011560879"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["paramscoresdf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">activation</th>\n","      <th colspan=\"4\" halign=\"left\">lr</th>\n","      <th colspan=\"4\" halign=\"left\">epochs</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>elu</th>\n","      <th>relu6</th>\n","      <th>leaky_relu</th>\n","      <th>relu</th>\n","      <th>0.001</th>\n","      <th>0.01</th>\n","      <th>0.1</th>\n","      <th>1</th>\n","      <th>20</th>\n","      <th>40</th>\n","      <th>60</th>\n","      <th>80</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>01</th>\n","      <td>3.6215</td>\n","      <td>3.26488</td>\n","      <td>4.12335</td>\n","      <td>3.64576</td>\n","      <td>2.62177</td>\n","      <td>2.74448</td>\n","      <td>3.76998</td>\n","      <td>5.51926</td>\n","      <td>4.53114</td>\n","      <td>3.45129</td>\n","      <td>3.33585</td>\n","      <td>3.3372</td>\n","    </tr>\n","    <tr>\n","      <th>02</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Mean</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Std.</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     activation                       ...     epochs                 \n","            elu    relu6 leaky_relu   ...         40       60      80\n","01       3.6215  3.26488    4.12335   ...    3.45129  3.33585  3.3372\n","02          NaN      NaN        NaN   ...        NaN      NaN     NaN\n","Mean        NaN      NaN        NaN   ...        NaN      NaN     NaN\n","Std.        NaN      NaN        NaN   ...        NaN      NaN     NaN\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":29}]}]}